{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c27a3b-7deb-4d18-8adc-b4def0beec22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Configuración\n",
    "base_url = \"https://www.datos.gov.co/resource/rpmr-utcd.csv\"\n",
    "limit = 100000\n",
    "total_registros = 16000000\n",
    "max_iter = total_registros // limit\n",
    "\n",
    "# Obtener esquema de la tabla destino\n",
    "target_schema = spark.table(\"main.diplomado_datos.secop\").schema\n",
    "\n",
    "# Descarga por lotes\n",
    "for i in range(max_iter):\n",
    "    offset = i * limit\n",
    "    print(f\"⏳ Iteración {i+1}/{max_iter} — Offset: {offset}\")\n",
    "\n",
    "    url = f\"{base_url}?$limit={limit}&$offset={offset}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"❌ Error al descargar en offset {offset}. Código: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    if len(response.text.strip()) == 0:\n",
    "        print(\"✅ No hay más datos disponibles.\")\n",
    "        break\n",
    "\n",
    "    # ✅ Corrección aquí: forzar todas las columnas a tipo texto\n",
    "    df_pd = pd.read_csv(StringIO(response.text), delimiter=\",\", header=0, dtype=str, low_memory=False)\n",
    "\n",
    "    if df_pd.empty:\n",
    "        print(\"✅ Descarga terminada: datos vacíos.\")\n",
    "        break\n",
    "\n",
    "    # Convertir a Spark y alinear tipos con el esquema destino\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "    df_aligned = df_spark.select(\n",
    "        [col(field.name).cast(field.dataType) for field in target_schema.fields if field.name in df_spark.columns]\n",
    "    )\n",
    "\n",
    "    # Guardar lote en tabla Delta\n",
    "    df_aligned.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "    print(f\"✅ Guardados {df_pd.shape[0]} registros. Total acumulado: {(i+1)*limit}\")\n",
    "\n",
    "print(\"\uD83C\uDF89 Proceso completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "079fe1c1-7c70-40d4-b6eb-6bd4b2c002f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar registros en la tabla Delta\n",
    "df_secop = spark.table(\"main.diplomado_datos.secop\")\n",
    "total_registros = df_secop.count()\n",
    "\n",
    "print(f\"\uD83D\uDD0E Total de registros en 'main.diplomado_datos.secop': {total_registros:,}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "INGESTA_DE_DATOS_SECOP",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}