{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "736ff7bd-fd5b-48b0-97c3-2a52c8dff80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cuaderno ingesta de datos \n",
    "\n",
    "En este cuaderno traeremos la informaci√≤n de datos abiertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2136a6-47ab-41cb-b8e2-b5729de00ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos con requests y leerlos en pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv\"\n",
    "url_men = \"https://www.datos.gov.co/resource/nudc-7mev.csv\"\n",
    "\n",
    "# Descargar contenido\n",
    "response_secop = requests.get(url_secop)\n",
    "response_men = requests.get(url_men)\n",
    "\n",
    "# Convertir contenido a pandas usando StringIO\n",
    "df_secop_pd = pd.read_csv(StringIO(response_secop.text))\n",
    "df_men_pd = pd.read_csv(StringIO(response_men.text))\n",
    "\n",
    "# Convertir pandas a Spark\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "df_men = spark.createDataFrame(df_men_pd)\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_secop)\n",
    "display(df_men)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde1a9cb-b9ea-4db7-8f2a-832bfa9501da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_secop_pd.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df075af-8ab4-407e-bfca-c6041a7c36b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_men_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4451e2f8-d494-4ae1-9510-bfba75d20bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Celda 2: Guardar los DataFrames como tablas Delta\n",
    "# La funci√≥n .saveAsTable() guarda los datos y registra la tabla en el Unity Catalog.\n",
    "# El modo \"overwrite\" reemplaza la tabla si ya existe, ideal para actualizaciones.\n",
    "df_secop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.secop\")\n",
    "df_men.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.men_estadisticas\")\n",
    "\n",
    "print(\"¬°Tablas guardadas exitosamente en el cat√°logo 'main', esquema 'diplomado_datos'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba30ba52-5f66-4863-af3e-18d3ccac8ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Configuraci√≥n\n",
    "base_url = \"https://www.datos.gov.co/resource/rpmr-utcd.csv\"\n",
    "limit = 100000\n",
    "total_registros = 16000000\n",
    "max_iter = total_registros // limit\n",
    "\n",
    "# Obtener esquema de la tabla destino\n",
    "target_schema = spark.table(\"main.diplomado_datos.secop\").schema\n",
    "\n",
    "# Descarga por lotes\n",
    "for i in range(max_iter):\n",
    "    offset = i * limit\n",
    "    print(f\"‚è≥ Iteraci√≥n {i+1}/{max_iter} ‚Äî Offset: {offset}\")\n",
    "\n",
    "    url = f\"{base_url}?$limit={limit}&$offset={offset}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Error al descargar en offset {offset}. C√≥digo: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    if len(response.text.strip()) == 0:\n",
    "        print(\"‚úÖ No hay m√°s datos disponibles.\")\n",
    "        break\n",
    "\n",
    "    # ‚úÖ Correcci√≥n aqu√≠: forzar todas las columnas a tipo texto\n",
    "    df_pd = pd.read_csv(StringIO(response.text), delimiter=\",\", header=0, dtype=str, low_memory=False)\n",
    "\n",
    "    if df_pd.empty:\n",
    "        print(\"‚úÖ Descarga terminada: datos vac√≠os.\")\n",
    "        break\n",
    "\n",
    "    # Convertir a Spark y alinear tipos con el esquema destino\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "    df_aligned = df_spark.select(\n",
    "        [col(field.name).cast(field.dataType) for field in target_schema.fields if field.name in df_spark.columns]\n",
    "    )\n",
    "\n",
    "    # Guardar lote en tabla Delta\n",
    "    df_aligned.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "    print(f\"‚úÖ Guardados {df_pd.shape[0]} registros. Total acumulado: {(i+1)*limit}\")\n",
    "\n",
    "print(\"üéâ Proceso completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d0ee99-78bb-4c10-be0f-d9cce3ecee28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar registros en la tabla Delta\n",
    "df_secop = spark.table(\"main.diplomado_datos.secop\")\n",
    "total_registros = df_secop.count()\n",
    "\n",
    "print(f\"üîé Total de registros en 'main.diplomado_datos.secop': {total_registros:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0498fa02-ed1d-4149-879a-dbde43f8ffae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar primeras filas\n",
    "display(df_secop.limit(10))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Datos_Abiertos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
